{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "from neural_nets_library import training\n",
    "from tree_to_sequence.tree_decoder import TreeDecoder\n",
    "from tree_to_sequence.tree_to_tree_attention import TreeToTreeAttention\n",
    "from tree_to_sequence.program_datasets import *\n",
    "from functools import partial\n",
    "from math_expressions.translating_math_trees import math_tokens\n",
    "from tree_to_sequence.translating_trees import pretty_print_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = False\n",
    "image_width = 40\n",
    "image_height = 32\n",
    "one_hot = False\n",
    "binarize_output = False\n",
    "eos_token = False\n",
    "long_base_case = True\n",
    "output_as_seq = False\n",
    "num_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sketchy_list(tree):\n",
    "    if type(tree) == list:\n",
    "        print(\"something is weird here\")\n",
    "        print(tree)\n",
    "        return True\n",
    "    else:\n",
    "        child_is_weird = True in [find_sketchy_list(child) for child in tree.children]\n",
    "        if child_is_weird:\n",
    "            print(tree.value)\n",
    "        return False\n",
    "        \n",
    "find_sketchy_list(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(train_dset[0][1])\n",
    "pretty_print_tree(train_dset[0][1])\n",
    "print(train_dset[0][1].size())\n",
    "i = 0\n",
    "for (img, tree) in train_dset:\n",
    "    try:\n",
    "        size = tree.size()\n",
    "        i += 1\n",
    "    except:\n",
    "        print(\"I is \", i)\n",
    "        print(\"tree?\", tree)\n",
    "        pretty_print_tree(tree)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('math_expressions/data_new/train_data.pkl', 'rb') as f:\n",
    "    train_dset = pickle.load(f)\n",
    "with open('math_expressions/data_new/test_data.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "    \n",
    "def process_img(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img[:image_height, :image_width]\n",
    "    img = torch.FloatTensor(img).unsqueeze(0).unsqueeze(0)\n",
    "    return img\n",
    "    \n",
    "train_dset = [(process_img(x), encode_program(y, math_tokens)) for x,y in train_dset]\n",
    "\n",
    "print(\"img shape\", train_dset[0][0].shape)\n",
    "\n",
    "# test_dset = ForLambdaDataset(\"ANC/AdditionalForDatasets/ForWithLevels/Easy-arbitraryForList.json\", binarize_input=True, \n",
    "#                                    binarize_output=binarize_output, eos_token=eos_token, one_hot=one_hot, \n",
    "#                                    long_base_case=long_base_case, input_as_seq=True, \n",
    "#                                    output_as_seq=output_as_seq, num_samples=30)\n",
    "\n",
    "\n",
    "\n",
    "max_size = 100\n",
    "# max_size = max([x[1].size() for x in train_dset])\n",
    "# print(\"max size\", max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(val, ops): \n",
    "    \"\"\"\n",
    "        Based on the value, num_variables, num_ints, and the possible ops, the index corresponding\n",
    "        to the value is found. value should not correspond to the eos_token. Instead vectorization\n",
    "        should occur prior to adding eos_tokens. Nodes with value None are simply returned as None.\n",
    "    \"\"\"\n",
    "    num_ints = 10\n",
    "    num_vars = 26\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwzyz\"\n",
    "\n",
    "\n",
    "    if type(val) is int:\n",
    "        index = val % num_ints\n",
    "    elif val not in ops:\n",
    "        index = alphabet.index(val) + num_ints\n",
    "    else:\n",
    "        index = num_ints + num_vars + ops[val]\n",
    "\n",
    "    return torch.LongTensor([index])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def encode_program(program, ops):\n",
    "    if isinstance(program, Node):\n",
    "        return map_tree(lambda node: vectorize(node, ops), program)\n",
    "    else:\n",
    "        program = map(lambda node: vectorize(node, ops), program)\n",
    "        return torch.LongTensor(list(program))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, nchannels, nhidden, num_layers, attention=True):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.core = nn.Sequential(CNN_Sequence_Extractor(nchannels), nn.LSTM(512, nhidden, num_layers, bidirectional=True))\n",
    "        self.register_buffer('reverse_indices', torch.LongTensor(range(1, num_layers*2, 2)))\n",
    "        \n",
    "        self.attention = attention\n",
    "\n",
    "    def forward(self, input, widths=None):\n",
    "        output, (all_hiddens, all_cell_state) = self.core(input)#, widths=widths) #TODO: Figure out what widths once did\n",
    "\n",
    "        if widths is not None:\n",
    "              output = nn.utils.rnn.pad_packed_sequence(output)\n",
    "\n",
    "        forward_hiddens = all_hiddens.index_select(0, self.reverse_indices - 1)\n",
    "        reverse_hiddens = all_hiddens.index_select(0, self.reverse_indices) #TODO: does this need a gradient\n",
    "        hiddens = torch.cat([forward_hiddens, reverse_hiddens], 2)\n",
    "        \n",
    "        forward_cell_state = all_cell_state.index_select(0, self.reverse_indices - 1)\n",
    "        reverse_cell_state = all_cell_state.index_select(0, self.reverse_indices) #TODO: does this need a gradient\n",
    "        cell_state = torch.cat([forward_cell_state, reverse_cell_state], 2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.attention:\n",
    "              return output.squeeze(1), hiddens.squeeze(0), cell_state.squeeze(0)\n",
    "        else:\n",
    "              return reverse_hiddens\n",
    "            \n",
    "            \n",
    "class CNN_Sequence_Extractor(nn.Module):\n",
    "    def __init__(self, nchannels, leakyRelu=False):\n",
    "        super(CNN_Sequence_Extractor, self).__init__()\n",
    "\n",
    "        # Size of the kernel (image filter) for each convolutional layer.\n",
    "        ks = [3, 3, 3, 3, 3, 3, 2]\n",
    "        # Amount of zero-padding for each convoutional layer.\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
    "        # The stride for each convolutional layer. The list elements are of the form (height stride, width stride).\n",
    "        ss = [(2,2), (2,2), (1,1), (2,1), (1,1), (2,1), (1,1)]\n",
    "        # Number of channels in each convolutional layer.\n",
    "        nm = [64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "        # Initializing the container for the modules that make up the neural network the neurel netowrk.\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        # Represents a convolutional layer. The input paramter i signals that this is the ith convolutional layer. The user also has the option to set batchNormalization to True which will perform a batch normalization on the image after it has undergone a convoltuional pass. There is no output but this function adds the convolutional layer module created here to the sequential container, cnn.\n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = nchannels if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                cnn.add_module('leaky_relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        # Creating the 7 convolutional layers for the model.\n",
    "        convRelu(0)\n",
    "        convRelu(1)\n",
    "        convRelu(2, True)\n",
    "        convRelu(3)\n",
    "        convRelu(4, True)\n",
    "        convRelu(5)\n",
    "        convRelu(6, True)\n",
    "\n",
    "        self.cnn = cnn\n",
    "\n",
    "    def forward(self, input, widths=None):\n",
    "        output = self.cnn(input)\n",
    "        _, _, h, _ = output.size()\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        output = output.squeeze(2) # [b, c, w]\n",
    "        output = output.permute(2, 0, 1) #[w, b, c]\n",
    "\n",
    "        if widths is not None:\n",
    "            sorted_widths, idx = widths.sort(descending=True)\n",
    "            output = output.index_select(1, idx)\n",
    "            output = nn.utils.pack_padded_sequence(output, sorted_widths / 4)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_all_parameters_uniform(model, stdev):\n",
    "    for param in model.parameters():\n",
    "        nn.init.uniform_(param, -stdev, stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_bonus = 1 if eos_token else 0\n",
    "nclass = len(math_tokens) # TODO: FIGURE THIS OUT\n",
    "plot_every = 100\n",
    "max_num_children = 4\n",
    "hidden_size = 256\n",
    "embedding_size = 25\n",
    "alignment_size = 100\n",
    "n_channels = 1\n",
    "num_layers = 1 # TODO: Later consider making this work for num_layers > 1\n",
    "max_size = 100 # TODO: FIGURE THIS OUT    \n",
    "align_type = 1\n",
    "    \n",
    "encoder = ImageEncoder(n_channels, hidden_size, num_layers, attention=True)\n",
    "decoder = TreeDecoder(embedding_size, hidden_size * 2, max_num_children, nclass=nclass)\n",
    "program_model = TreeToTreeAttention(encoder, decoder, hidden_size * 2, embedding_size, nclass=nclass, max_size=max_size,\n",
    "                                    alignment_size=alignment_size, align_type=align_type)\n",
    "    \n",
    "    \n",
    "reset_all_parameters_uniform(program_model, 0.1)\n",
    "decoder.initialize_forget_bias(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    program_model = program_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(program_model.parameters(), lr=0.005)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=500, factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of matches between the prediction and target.\n",
    "def count_matches(prediction, target):\n",
    "    matches = 0\n",
    "    if int(prediction.value) == int(target.value):\n",
    "        matches += 1\n",
    "    for i in range(min(len(target.children), len(prediction.children))):\n",
    "        matches += count_matches(prediction.children[i], target.children[i])\n",
    "    return matches\n",
    "\n",
    "# Program accuracy (1 if completely correct, 0 otherwise)\n",
    "def program_accuracy(prediction, target):\n",
    "    if prediction.size() == count_matches(prediction, target) and \\\n",
    "       prediction.size() == target.size():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Calculate validation accuracy (this could either be program or token accuracy)\n",
    "def validation_criterion(prediction, target):\n",
    "    return program_accuracy(prediction, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model, train_plot_losses, validation_plot_losses, _, _ = training.train_model_tree_to_tree(program_model, train_dset, \n",
    "                                 optimizer, lr_scheduler=None, num_epochs=5, plot_every=plot_every,\n",
    "                                 batch_size=1, print_every=200, validation_criterion=validation_criterion,\n",
    "                                 use_cuda=use_cuda)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x * plot_every for x in range(len(train_plot_losses))], train_plot_losses)\n",
    "plt.show()\n",
    "\n",
    "plt.plot([x * plot_every for x in range(len(validation_plot_losses))], validation_plot_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.tensor(np.random.rand(1,100,256))\n",
    "r.index_select(1, torch.tensor([1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
