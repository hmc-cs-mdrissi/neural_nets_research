{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mehdi2277/Documents/HarveyMuddWork/Neural_Nets_Research/neural_nets_research\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from neural_nets_library import training\n",
    "from tree_to_sequence.tree_decoder import TreeDecoder\n",
    "from tree_to_sequence.tree_to_tree_attention import TreeToTreeAttention\n",
    "from tree_to_sequence.program_datasets import *\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = False\n",
    "image_width = 40\n",
    "image_height = 32\n",
    "one_hot = False\n",
    "binarize_output = False\n",
    "eos_token = False\n",
    "long_base_case = True\n",
    "output_as_seq = False\n",
    "num_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset = ForLambdaDataset(\"ANC/AdditionalForDatasets/ForWithLevels/Easy-arbitraryForList.json\", binarize_input=True, \n",
    "                                   binarize_output=binarize_output, eos_token=eos_token, one_hot=one_hot, \n",
    "                                   long_base_case=long_base_case, input_as_seq=True, \n",
    "                                   output_as_seq=output_as_seq, num_samples=100)\n",
    "\n",
    "\n",
    "test_dset = [(torch.ones((1, 1, image_height, image_width)), output_image) for (input_image, output_image) in test_dset.program_pairs]\n",
    "\n",
    "\n",
    "max_size = max([x[1].size() for x in test_dset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, nchannels, nhidden, num_layers, attention=True):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.core = nn.Sequential(CNN_Sequence_Extractor(nchannels), nn.LSTM(512, nhidden, num_layers, bidirectional=True))\n",
    "        self.register_buffer('reverse_indices', torch.LongTensor(range(1, num_layers*2, 2)))\n",
    "        print(\"reverse indices size is\", self.reverse_indices.shape)\n",
    "        self.attention = attention\n",
    "\n",
    "    def forward(self, input, widths=None):\n",
    "        output, (all_hiddens, _) = self.core(input)#, widths=widths) #TODO: Figure out what widths once did\n",
    "\n",
    "        if widths is not None:\n",
    "              output = nn.utils.rnn.pad_packed_sequence(output)\n",
    "\n",
    "        reverse_hiddens = all_hiddens.index_select(0, self.reverse_indices) #TODO: does this need a gradient\n",
    "        all_hiddens[-2:]\n",
    "        print(\"all hiddens shape\", all_hiddens.shape)\n",
    "        print(\"reverse hiddens\", reverse_hiddens.shape)\n",
    "        \n",
    "        if self.attention:\n",
    "              return output.squeeze(1), reverse_hiddens.squeeze(1), torch.zeros(2,3)\n",
    "        else:\n",
    "              return reverse_hiddens\n",
    "            \n",
    "            \n",
    "class CNN_Sequence_Extractor(nn.Module):\n",
    "    def __init__(self, nchannels, leakyRelu=False):\n",
    "        super(CNN_Sequence_Extractor, self).__init__()\n",
    "\n",
    "        # Size of the kernel (image filter) for each convolutional layer.\n",
    "        ks = [3, 3, 3, 3, 3, 3, 2]\n",
    "        # Amount of zero-padding for each convoutional layer.\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
    "        # The stride for each convolutional layer. The list elements are of the form (height stride, width stride).\n",
    "        ss = [(2,2), (2,2), (1,1), (2,1), (1,1), (2,1), (1,1)]\n",
    "        # Number of channels in each convolutional layer.\n",
    "        nm = [64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "        # Initializing the container for the modules that make up the neural network the neurel netowrk.\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        # Represents a convolutional layer. The input paramter i signals that this is the ith convolutional layer. The user also has the option to set batchNormalization to True which will perform a batch normalization on the image after it has undergone a convoltuional pass. There is no output but this function adds the convolutional layer module created here to the sequential container, cnn.\n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = nchannels if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                cnn.add_module('leaky_relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        # Creating the 7 convolutional layers for the model.\n",
    "        convRelu(0)\n",
    "        convRelu(1)\n",
    "        convRelu(2, True)\n",
    "        convRelu(3)\n",
    "        convRelu(4, True)\n",
    "        convRelu(5)\n",
    "        convRelu(6, True)\n",
    "\n",
    "        self.cnn = cnn\n",
    "\n",
    "    def forward(self, input, widths=None):\n",
    "        print(\"input is\", input)\n",
    "        print(\"input size\", input.shape)\n",
    "        output = self.cnn(input)\n",
    "        _, _, h, _ = output.size()\n",
    "        print(\"output size\", output.size())\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        output = output.squeeze(2) # [b, c, w]\n",
    "        output = output.permute(2, 0, 1) #[w, b, c]\n",
    "\n",
    "        if widths is not None:\n",
    "            sorted_widths, idx = widths.sort(descending=True)\n",
    "            output = output.index_select(1, idx)\n",
    "            output = nn.utils.pack_padded_sequence(output, sorted_widths / 4)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_all_parameters_uniform(model, stdev):\n",
    "    for param in model.parameters():\n",
    "        nn.init.uniform_(param, -stdev, stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reverse indices size is torch.Size([1])\n",
      "second case, dimensions are 512 512\n"
     ]
    }
   ],
   "source": [
    "eos_bonus = 1 if eos_token else 0\n",
    "nclass = 50 # TODO: FIGURE THIS OUT\n",
    "plot_every = 100\n",
    "max_num_children = 4\n",
    "hidden_size = 256\n",
    "embedding_size = 25\n",
    "alignment_size = 100\n",
    "n_channels = 1\n",
    "num_layers = 1 # TODO: Later consider making this work for num_layers > 1\n",
    "max_size = 100 # TODO: FIGURE THIS OUT    \n",
    "align_type = 1\n",
    "    \n",
    "encoder = ImageEncoder(n_channels, hidden_size, num_layers, attention=True)\n",
    "decoder = TreeDecoder(embedding_size, hidden_size * 2, max_num_children, nclass=nclass)\n",
    "program_model = TreeToTreeAttention(encoder, decoder, hidden_size * 2, embedding_size, nclass=nclass, max_size=max_size,\n",
    "                                    alignment_size=alignment_size, align_type=align_type)\n",
    "    \n",
    "    \n",
    "reset_all_parameters_uniform(program_model, 0.1)\n",
    "decoder.initialize_forget_bias(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    program_model = program_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(program_model.parameters(), lr=0.005)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=500, factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of matches between the prediction and target.\n",
    "def count_matches(prediction, target):\n",
    "    matches = 0\n",
    "    if int(prediction.value) == int(target.value):\n",
    "        matches += 1\n",
    "    for i in range(min(len(target.children), len(prediction.children))):\n",
    "        matches += count_matches(prediction.children[i], target.children[i])\n",
    "    return matches\n",
    "\n",
    "# Program accuracy (1 if completely correct, 0 otherwise)\n",
    "def program_accuracy(prediction, target):\n",
    "    if prediction.size() == count_matches(prediction, target) and \\\n",
    "       prediction.size() == target.size():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Calculate validation accuracy (this could either be program or token accuracy)\n",
    "def validation_criterion(prediction, target):\n",
    "    return program_accuracy(prediction, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "input size torch.Size([100, 1, 32, 40])\n",
      "output size torch.Size([100, 512, 1, 9])\n",
      "all hiddens shape torch.Size([2, 100, 256])\n",
      "reverse hiddens torch.Size([1, 100, 256])\n",
      "torch.Size([9, 100, 512])\n",
      "torch.Size([100, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input = torch.zeros((100, 1, 32, 40))\n",
    "\n",
    "attention_matrix , encoding, _ = encoder(input)\n",
    "encoding = encoding.squeeze(0)\n",
    "\n",
    "print(attention_matrix.shape)\n",
    "print(encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "input is tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
      "input size torch.Size([1, 1, 32, 40])\n",
      "output size torch.Size([1, 512, 1, 9])\n",
      "all hiddens shape torch.Size([2, 1, 256])\n",
      "reverse hiddens torch.Size([1, 1, 256])\n",
      "dimesnsions of things the encoder returns: torch.Size([9, 512]) torch.Size([1, 256]) torch.Size([2, 3])\n",
      "annotations shape!!! torch.Size([9, 512])\n",
      "attention hidden values torch.Size([9, 512])\n",
      "decoder hiddens torch.Size([1, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-eb68a532a7a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                  \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplot_every\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_criterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_criterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                  use_cuda=use_cuda)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HarveyMuddWork/Neural_Nets_Research/neural_nets_research/neural_nets_library/training.py\u001b[0m in \u001b[0;36mtrain_model_tree_to_tree\u001b[0;34m(model, dset_loader, optimizer, lr_scheduler, num_epochs, print_every, plot_every, batch_size, validation_criterion, validation_dset, plateau_lr, use_cuda, save_file, save_folder, save_every)\u001b[0m\n\u001b[1;32m    594\u001b[0m             \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0miteration_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0miteration_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HarveyMuddWork/Neural_Nets_Research/neural_nets_research/tree_to_sequence/tree_to_tree_attention.py\u001b[0m in \u001b[0;36mforward_train\u001b[0;34m(self, input_tree, target_tree, teacher_forcing)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# Use attention and past hidden state to generate scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_hiddens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mattention_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_hidden_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_logits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# number_of_nodes x 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mcontext_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1 x hidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HarveyMuddWork/Neural_Nets_Research/neural_nets_research/tree_to_sequence/tree_to_tree_attention.py\u001b[0m in \u001b[0;36mattention_logits\u001b[0;34m(self, attention_hidden_values, decoder_hidden)\u001b[0m\n\u001b[1;32m    192\u001b[0m                                                              + attention_hidden_values))\n\u001b[1;32m    193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mattention_hidden_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_max_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "best_model, train_plot_losses, validation_plot_losses = training.train_model_tree_to_tree(program_model, test_dset, \n",
    "                                 optimizer, lr_scheduler=None, num_epochs=5, plot_every=plot_every,\n",
    "                                 batch_size=100, print_every=200, validation_criterion=validation_criterion,\n",
    "                                 use_cuda=use_cuda)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxxxx = torch.zeros((3,2))\n",
    "print(xxxxx)\n",
    "print(xxxxx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x * plot_every for x in range(len(train_plot_losses))], train_plot_losses)\n",
    "plt.show()\n",
    "\n",
    "plt.plot([x * plot_every for x in range(len(validation_plot_losses))], validation_plot_losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
