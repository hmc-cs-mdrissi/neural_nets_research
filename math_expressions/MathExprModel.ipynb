{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "from neural_nets_library import training\n",
    "from tree_to_sequence.tree_decoder_batch import TreeDecoderBatch\n",
    "from tree_to_sequence.tree_to_tree_attention import TreeToTreeAttention\n",
    "from tree_to_sequence.tree_to_tree_attention_batch import TreeToTreeAttentionBatch\n",
    "from tree_to_sequence.program_datasets import *\n",
    "from functools import partial\n",
    "from math_expressions.translating_math_trees import math_tokens_short as math_tokens\n",
    "from tree_to_sequence.translating_trees import pretty_print_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "image_width = 40\n",
    "image_height = 32\n",
    "one_hot = False\n",
    "binarize_output = True\n",
    "eos_token = True\n",
    "long_base_case = True\n",
    "output_as_seq = False\n",
    "num_samples = None\n",
    "max_num_children = 2 if binarize_output else 3\n",
    "batch_size = 5\n",
    "normalize_input = True\n",
    "num_layers = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, split):\n",
    "    all_trees = []\n",
    "    for img, tree in data:\n",
    "        if not tree in all_trees:\n",
    "            all_trees.append(tree)\n",
    "    \n",
    "    split_cutoff = int(len(all_trees) * split)\n",
    "    first_split = all_trees[:split_cutoff]\n",
    "    second_split = all_trees[split_cutoff:]\n",
    "    \n",
    "    first_data = [(img, tree) for img, tree in data if tree in first_split]\n",
    "    second_data = [(img, tree) for img, tree in data if tree in second_split]\n",
    "    \n",
    "    return first_data, second_data\n",
    "\n",
    "\n",
    "def make_dset(data):\n",
    "    return MathExprDatasetBatched(program_pairs=data, \n",
    "                                 batch_size=batch_size,\n",
    "                                 binarize_output=binarize_output,\n",
    "                                 validation_set=False,\n",
    "                                 max_children_output=max_num_children,\n",
    "                                 eos_token=eos_token,\n",
    "                                 normalize=normalize_input,\n",
    "                                 num_samples=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split into train/val/test sets\n",
    "test_data = load_shuffled_data(\"math_expressions/test_data_short.pkl\")\n",
    "all_data = load_shuffled_data(\"math_expressions/train_data_short.pkl\")\n",
    "\n",
    "train_cutoff = 0.7\n",
    "train_data, val_data = split_dataset(all_data, train_cutoff)\n",
    "\n",
    "print(\"Train set size: \", len(train_data))\n",
    "print(\"Val set size: \", len(val_data))\n",
    "print(\"Test set size: \", len(test_data))\n",
    "\n",
    "train_dset = make_dset(train_data)\n",
    "val_dset = make_dset(val_data)\n",
    "# test_dset = make_dset(test_data)\n",
    "\n",
    "max_size = max([tree.size() for batch in train_dset for tree in batch[1]])\n",
    "print(\"max size\", max_size)\n",
    "print(len(train_dset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_normally(pic, title=None):\n",
    "        if not title is None:\n",
    "            plt.title(title)\n",
    "        plt.imshow(np.repeat(np.int0(pic)[:,:,np.newaxis]*255, 3, axis=2))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Print the dset\n",
    "# for batched_img, batched_trees in train_dset:\n",
    "#     for i in range(len(batched_img)):\n",
    "#         img = batched_img[i]\n",
    "#         tree = batched_trees[i]\n",
    "#         display_normally(img[0])\n",
    "#         pretty_print_tree(tree, math_tokens)\n",
    "#         pretty_print_tree(tree)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, nchannels, nhidden, num_layers, num_cnn_layers, attention=True):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.core = nn.Sequential(CNN_Sequence_Extractor(nchannels, num_cnn_layers), nn.LSTM(512, nhidden, num_layers, bidirectional=True))\n",
    "        self.register_buffer('reverse_indices', torch.LongTensor(range(1, num_layers*2, 2)))\n",
    "        \n",
    "        self.attention = attention\n",
    "\n",
    "    def forward(self, input, widths=None, training=True):\n",
    "        output, (all_hiddens, all_cell_state) = self.core(input)#, widths=widths) #TODO: Figure out what widths once did\n",
    "\n",
    "        if widths is not None:\n",
    "              output = nn.utils.rnn.pad_packed_sequence(output)\n",
    "\n",
    "        forward_hiddens = all_hiddens.index_select(0, self.reverse_indices - 1)\n",
    "        reverse_hiddens = all_hiddens.index_select(0, self.reverse_indices) #TODO: does this need a gradient\n",
    "        hiddens = torch.cat([forward_hiddens, reverse_hiddens], 2)\n",
    "        \n",
    "        forward_cell_state = all_cell_state.index_select(0, self.reverse_indices - 1)\n",
    "        reverse_cell_state = all_cell_state.index_select(0, self.reverse_indices) #TODO: does this need a gradient\n",
    "        cell_state = torch.cat([forward_cell_state, reverse_cell_state], 2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.attention:        \n",
    "            if training:\n",
    "                return output, hiddens.squeeze(0), cell_state.squeeze(0) # TODO: This is here b/c currently training is batched but testing isn't.  Someday we should fix this\n",
    "            return output.squeeze(1), hiddens.squeeze(0), cell_state.squeeze(0)\n",
    "        else:\n",
    "              return reverse_hiddens\n",
    "            \n",
    "            \n",
    "class CNN_Sequence_Extractor(nn.Module):\n",
    "    def __init__(self, nchannels, num_layers, leakyRelu=False):\n",
    "        super(CNN_Sequence_Extractor, self).__init__()\n",
    "\n",
    "#         # ORIGINAL MODEL SIZE\n",
    "#         ks = [3, 3, 3, 3, 3, 3, 2]\n",
    "#         ps = [1, 1, 1, 1, 1, 1, 0]\n",
    "#         ss = [(2,2), (3,2), (2,1), (3,1), (2,1), (3,1), (2,1)]\n",
    "#         nm = [64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "        assert(num_layers >= 4)\n",
    "\n",
    "        # Size of the kernel (image filter) for each convolutional layer.\n",
    "        ks = [3] * (num_layers - 1) + [2]\n",
    "        \n",
    "        # Amount of zero-padding for each convoutional layer.\n",
    "        ps = [1] * (num_layers - 1) + [0]\n",
    "\n",
    "        # The stride for each convolutional layer. The list elements are of the form (height stride, width stride).\n",
    "        ss = [(2,2), (3,2)] + [(2,1) if i % 2 else (3,1) for i in range(num_layers - 2)]\n",
    "        \n",
    "        # Number of channels in each convolutional layer.\n",
    "        nm = [64, 128, 245, 256] + [512] * (num_layers - 4)\n",
    "\n",
    "        # Initializing the container for the modules that make up the neural network the neurel netowrk.\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        # Represents a convolutional layer. The input paramter i signals that this is the ith convolutional layer. The user also has the option to set batchNormalization to True which will perform a batch normalization on the image after it has undergone a convoltuional pass. There is no output but this function adds the convolutional layer module created here to the sequential container, cnn.\n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = nchannels if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                cnn.add_module('leaky_relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        batch_norm_on = True\n",
    "        # Creating the 7 convolutional layers for the model.\n",
    "        convRelu(0)\n",
    "        convRelu(1)\n",
    "        convRelu(2, batch_norm_on)\n",
    "        convRelu(3)\n",
    "        convRelu(4, batch_norm_on)\n",
    "        convRelu(5)\n",
    "        convRelu(6, batch_norm_on)\n",
    "\n",
    "        self.cnn = cnn\n",
    "\n",
    "    def forward(self, input, widths=None):\n",
    "        output = self.cnn(input)\n",
    "        _, _, h, _ = output.size()\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        output = output.squeeze(2) # [b, c, w]\n",
    "        output = output.permute(2, 0, 1) #[w, b, c]\n",
    "\n",
    "        if widths is not None:\n",
    "            sorted_widths, idx = widths.sort(descending=True)\n",
    "            output = output.index_select(1, idx)\n",
    "            output = nn.utils.pack_padded_sequence(output, sorted_widths / 4)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_all_parameters_uniform(model, stdev):\n",
    "    for param in model.parameters():\n",
    "        nn.init.uniform_(param, -stdev, stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_bonus = 1 if eos_token else 0\n",
    "nclass = len(math_tokens) + 26*2 + 10 # TODO: FIGURE THIS OUT\n",
    "plot_every = 100\n",
    "hidden_size = 256\n",
    "embedding_size = 100\n",
    "alignment_size = 50\n",
    "n_channels = 1\n",
    "num_layers = 1 # TODO: Later consider making this work for num_layers > 1\n",
    "align_type = 1\n",
    "num_cnn_layers = 16\n",
    "    \n",
    "encoder = ImageEncoder(n_channels, hidden_size, num_layers, num_cnn_layers, attention=True)\n",
    "decoder = TreeDecoderBatch(embedding_size, hidden_size*2, max_num_children, nclass=nclass)\n",
    "program_model = TreeToTreeAttentionBatch(encoder, decoder, hidden_size * 2, embedding_size, nclass=nclass, max_size=max_size,\n",
    "                                    alignment_size=alignment_size, align_type=align_type, use_cuda=use_cuda)    \n",
    "    \n",
    "reset_all_parameters_uniform(program_model, 0.1)\n",
    "decoder.initialize_forget_bias(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_model.decoder.EOS_value = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    program_model = program_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(program_model.parameters(), lr=0.001) #0.001\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=500, factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of matches between the prediction and target.\n",
    "def count_matches(prediction, target):\n",
    "    matches = 0\n",
    "    if int(prediction.value) == int(target.value):\n",
    "        matches += 1\n",
    "    for i in range(min(len(target.children), len(prediction.children))):\n",
    "        matches += count_matches(prediction.children[i], target.children[i])\n",
    "    return matches\n",
    "\n",
    "# Program accuracy (1 if completely correct, 0 otherwise)\n",
    "def program_accuracy(prediction, target):\n",
    "    target = target[0]\n",
    "    if prediction.size() == count_matches(prediction, target) and \\\n",
    "       prediction.size() == target.size():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Calculate validation accuracy (this could either be program or token accuracy)\n",
    "def validation_criterion(prediction, target):\n",
    "    return program_accuracy(prediction, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# program_model = torch.load(\"math_expressions/models/hopefully_fixed_model_sorted_correctly_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for input_tree, target_tree in train_dset[:20]:\n",
    "    input_tree = input_tree.cuda()\n",
    "    target_tree = [actual_tree.cuda() for actual_tree in target_tree]                    \n",
    "                        \n",
    "    program_model.eval()\n",
    "    program_model.print_img_tree_example(input_tree, target_tree, math_tokens)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model, train_plot_losses, train_plot_accuracies, _, _ = training.train_model_tree_to_tree(\n",
    "    program_model, \n",
    "    train_dset,                      \n",
    "    optimizer, \n",
    "    lr_scheduler=None, \n",
    "    num_epochs=100, \n",
    "    plot_every=plot_every,                            \n",
    "    batch_size=5, # 10+\n",
    "    print_every=20, \n",
    "    validation_criterion=validation_criterion, \n",
    "    validation_dset=val_dset,\n",
    "    save_folder =\"math_expressions/models\", \n",
    "    save_file=\"independent_val\",                        \n",
    "    use_cuda=use_cuda, \n",
    "    skip_output_cuda=False, \n",
    "    tokens=math_tokens,                     \n",
    "    save_current_only=True, \n",
    "    input_tree_form=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x * plot_every for x in range(len(train_plot_losses))], train_plot_losses)\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot([x * plot_every for x in range(len(train_plot_accuracies))], train_plot_accuracies)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cudafy_pair(pair):\n",
    "    img_cuda = pair[0].cuda()\n",
    "    tree_cuda = [tree.cuda() for tree in pair[1]]\n",
    "    return (img_cuda, tree_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_dset_cuda = [cudafy_pair(pair) for pair in val_dset]\n",
    "\n",
    "program_model.eval()\n",
    "acc = training.test_model_tree_to_tree(program_model, val_dset_cuda, validation_criterion, use_cuda=False) \n",
    "print(\"accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset_cuda = [cudafy_pair(pair) for pair in train_dset]\n",
    "\n",
    "program_model.eval()\n",
    "acc = training.test_model_tree_to_tree(program_model, train_dset_cuda, validation_criterion, use_cuda=False) \n",
    "print(\"accuracy\", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
