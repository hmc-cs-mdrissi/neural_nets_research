import torch
import torch.nn as nn

class TreeDecoderBatch(nn.Module):
    def __init__(self, embedding_size, hidden_size, max_num_children, nclass):
        """
        :param embedding_size: length of the encoded representation of a node
        :param hidden_size: hidden state size
        :param max_num_children: max. number of children a node can have
        :param nclass: number of different tokens which could be in a tree (not counting end
                       of tree token)
        """
        super(TreeDecoderBatch, self).__init__()
                
        self.cross_entropy = nn.CrossEntropyLoss(reduce=False)
        self.max_num_children = max_num_children
        
        # Linear layer to calculate log odds. The one is to account for the eos token.
        self.output_log_odds = nn.Linear(hidden_size, nclass + 1)        
        
        # Create a separate lstm for each child index
        self.lstm_list = nn.ModuleList()
        
        self.max_num_children = max_num_children #TODO: needed?
        
        for i in range(self.max_num_children):
            self.lstm_list.append(nn.LSTMCell(embedding_size + hidden_size, hidden_size)) #TODO: Maybe just to left/right
            
    def loss_func(self, a, b):
        """
        Compute loss.
        
        Wmy does this exist?
        """
        return self.cross_entropy(a.squeeze(1), b.squeeze(1))
    
    def calculate_loss(self, parent, child_index, et, true_value, print_time=False):
        """
        Calculate cross entropy loss from et.
        
        :param parent: node's parent (dummy; used for compatibility with grammar decoder)
        :param child_index: index of generated child (dummy; used for compatibility with grammar 
                            decoder)
        :param et: vector incorporating info from the attention and hidden state of past node
        :param true_value: true value of the new node
        :returns: cross entropy loss
        """
#         print("ET", et.shape, et[:5,0,:30])
        log_odds = self.output_log_odds(et)
#         print("LOG ODDS - TRAIN", log_odds.shape)
#         print(log_odds[0,0,:3])
#         assert False
#         print("shapes train", log_odds.shape)
#         print("true value shape", true_value.shape)
#         print("log odds TRAINING", log_odds)
#         _, max_index = torch.max(log_odds.squeeze(1), 1)
#         for i, vec in enumerate(print_time):
#             if vec:
#                 pass
#                 print("log odds TRAINING", log_odds.shape, log_odds[i])
#                 print("choosing TRAINING", max_index[i], "true val is", true_value[i])
        return self.loss_func(log_odds, true_value)
    
    
    #TODO: delete this???
    def get_next(self, parent, child_index, input, hidden_state, cell_state):
        """
        Generate the hidden and cell states which will be used to generate the current node's 
        children
        
        :param parent: node's parent (dummy; used for compatibility with grammar decoder)
        :param child_index: index of generated child
        :param input: embedded reprentation of the node's parent
        :param hidden_state: hidden state generated by an lstm
        :param cell_state: cell state generated by an lstm
        """
        return self.lstm_list[child_index](input, (hidden_state, cell_state))
    
    
    def number_children(self, parent):
        return self.max_num_children
    
    
    def get_next_child_states_left(self, parent, input, hidden_state, cell_state):
        """
        Generate the hidden and cell states which will be used to generate the current node's 
        children
        
        :param parent: node's parent (dummy; used for compatibility with grammar decoder)
        :param child_index: index of generated child 
        :param input: embedded reprentation of the node's parent
        :param hidden_state: hidden state generated by an lstm
        :param cell_state: cell state generated by an lstm
        :returns: hidden state and cell state of child node
        """
        return self.lstm_list[0](input.squeeze(1), (hidden_state, cell_state))
    
    
    def get_next_child_states_right(self, parent, input, hidden_state, cell_state):
        """
        Generate the hidden and cell states which will be used to generate the current node's 
        children
        
        :param parent: node's parent (dummy; used for compatibility with grammar decoder)
        :param child_index: index of generated child 
        :param input: embedded reprentation of the node's parent
        :param hidden_state: hidden state generated by an lstm
        :param cell_state: cell state generated by an lstm
        :returns: hidden state and cell state of child node
        """
        return self.lstm_list[1](input.squeeze(1), (hidden_state, cell_state))
    
    def initialize_forget_bias(self, bias_value):
        """
        Initialize the forget bias to a certain value. Primary purpose is that initializing
        with a largish value (like 3) tends to help convergence by preventing the model
        from forgetting too much early on.
        
        :param bias_value: value the forget bias wil be set to
        """
        for lstm in self.lstm_list:
            nn.init.constant_(lstm.bias_ih, bias_value)
            nn.init.constant_(lstm.bias_hh, bias_value)
            
    def make_prediction(self, parent, child_index, et, print_time=False):
        """
        Predict new child node value
        
        :param parent: parent node (dummy; used for compatibility with grammar decoder)
        :param child_index: index of generated child (dummy; used for compatibility with grammar decoder)
        :param et: attention vector of the parent
        """
#         print("ET", et.shape, et[0,:3])
        log_odds = self.output_log_odds(et)
#         print("LOG ODDS - Prediction", log_odds.shape)
#         print(log_odds[0,:3])
#         assert False
        _, max_index = torch.max(log_odds, 1)
        #         print("shapes test", log_odds.shape)
#         if print_time:
#             pass
#             print("log odds PREDICTION", log_odds)
#             print("choosing PREDICTION", max_index)
#         _, max_index = torch.max(log_odds[0], 1)
        return max_index