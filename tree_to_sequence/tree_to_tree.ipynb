{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcb34145d30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from neural_nets_library import training\n",
    "from tree_to_sequence.tree_encoder import TreeEncoder\n",
    "from tree_to_sequence.tree_decoder import TreeDecoder\n",
    "from tree_to_sequence.program_datasets import *\n",
    "from tree_to_sequence.translating_trees import *\n",
    "from tree_to_sequence.tree_to_tree_attention import TreeToTreeAttention\n",
    "from functools import partial\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "num_vars = 10\n",
    "num_ints = 11\n",
    "one_hot = False\n",
    "binarize_input = False\n",
    "binarize_output = False\n",
    "eos_token = False\n",
    "long_base_case = True\n",
    "input_as_seq = False\n",
    "output_as_seq = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "for_lambda_dset = ForLambdaDataset(\"ANC/AdditionalForDatasets/ForWithLevels/Easy-arbitraryForList.json\", binarize_input=binarize_input, \n",
    "                                   binarize_output=binarize_output, eos_token=eos_token, one_hot=one_hot, \n",
    "                                   long_base_case=long_base_case, input_as_seq=input_as_seq, \n",
    "                                   output_as_seq=output_as_seq, num_samples=10)\n",
    "max_size = max([x[1].size() for x in for_lambda_dset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([30])\n"
     ]
    }
   ],
   "source": [
    "print(for_lambda_dset[0][1].children[2].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ┌21┐\n",
      "   │  └12\n",
      " 28┤\n",
      "   └22┐\n",
      "      └9\n"
     ]
    }
   ],
   "source": [
    "pretty_print_tree(for_lambda_dset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ┌30\n",
      " 29┤\n",
      "   ├22┐\n",
      "   │  └9\n",
      "   └21┐\n",
      "      └12\n"
     ]
    }
   ],
   "source": [
    "pretty_print_tree(for_lambda_dset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_all_parameters_uniform(model, stdev):\n",
    "    for param in model.parameters():\n",
    "        nn.init.uniform_(param, -stdev, stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 100 #... 256 is from the paper, but 100 is WAY faster\n",
    "hidden_size = 3#256\n",
    "num_layers = 1\n",
    "alignment_size = 50\n",
    "align_type = 1\n",
    "encoder_input_size = num_vars + num_ints + len(for_ops)\n",
    "annotation_method = pre_order\n",
    "#Changed to randomize=true\n",
    "encoder = TreeEncoder(encoder_input_size, hidden_size, num_layers, [1, 2, 3, 4, 5], attention=True, one_hot=one_hot, binary_tree_lstm_cell=True, annotation_method=annotation_method, randomize_hiddens=False)\n",
    "nclass = num_vars + num_ints + len(lambda_ops)\n",
    "plot_every = 100\n",
    "max_num_children = 2 if binarize_output else 4\n",
    "\n",
    "decoder = TreeDecoder(embedding_size, hidden_size, max_num_children, nclass=nclass)\n",
    "program_model = TreeToTreeAttention(encoder, decoder, hidden_size, embedding_size, nclass=nclass, max_size=max_size,\n",
    "                                    alignment_size=alignment_size, align_type=align_type)\n",
    "    \n",
    "reset_all_parameters_uniform(program_model, 0.1)\n",
    "encoder.initialize_forget_bias(3)\n",
    "decoder.initialize_forget_bias(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_model = program_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(program_model.parameters(), lr=0.005)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=500, factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of matches between the prediction and target.\n",
    "def count_matches(prediction, target):\n",
    "    matches = 0\n",
    "    if int(prediction.value) == int(target.value):\n",
    "        matches += 1\n",
    "    for i in range(min(len(target.children), len(prediction.children))):\n",
    "        matches += count_matches(prediction.children[i], target.children[i])\n",
    "    return matches\n",
    "\n",
    "# Program accuracy (1 if completely correct, 0 otherwise)\n",
    "def program_accuracy(prediction, target):\n",
    "    if prediction.size() == count_matches(prediction, target) and \\\n",
    "       prediction.size() == target.size():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Calculate validation accuracy (this could either be program or token accuracy)\n",
    "def validation_criterion(prediction, target):\n",
    "    return program_accuracy(prediction, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_model.update_max_size(max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "Epoch 1/99\n",
      "----------\n",
      "Epoch Number: 1, Batch Number: 10, Training Loss: 26.6276\n",
      "Time so far is 0m 0s\n",
      "Epoch Number: 1, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 2/99\n",
      "----------\n",
      "Epoch 3/99\n",
      "----------\n",
      "Epoch Number: 3, Batch Number: 10, Training Loss: 26.6276\n",
      "Time so far is 0m 1s\n",
      "Epoch Number: 3, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 4/99\n",
      "----------\n",
      "Epoch 5/99\n",
      "----------\n",
      "Epoch Number: 5, Batch Number: 10, Training Loss: 26.6276\n",
      "Time so far is 0m 1s\n",
      "Epoch Number: 5, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 6/99\n",
      "----------\n",
      "Epoch 7/99\n",
      "----------\n",
      "Epoch Number: 7, Batch Number: 10, Training Loss: 26.6276\n",
      "Time so far is 0m 2s\n",
      "Epoch Number: 7, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 8/99\n",
      "----------\n",
      "Epoch 9/99\n",
      "----------\n",
      "Epoch Number: 9, Batch Number: 10, Training Loss: 26.5970\n",
      "Time so far is 0m 3s\n",
      "Epoch Number: 9, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 10/99\n",
      "----------\n",
      "Epoch 11/99\n",
      "----------\n",
      "Epoch Number: 11, Batch Number: 10, Training Loss: 26.5664\n",
      "Time so far is 0m 3s\n",
      "Epoch Number: 11, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 12/99\n",
      "----------\n",
      "Epoch 13/99\n",
      "----------\n",
      "Epoch Number: 13, Batch Number: 10, Training Loss: 26.5664\n",
      "Time so far is 0m 4s\n",
      "Epoch Number: 13, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 14/99\n",
      "----------\n",
      "Epoch 15/99\n",
      "----------\n",
      "Epoch Number: 15, Batch Number: 10, Training Loss: 26.5664\n",
      "Time so far is 0m 4s\n",
      "Epoch Number: 15, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 16/99\n",
      "----------\n",
      "Epoch 17/99\n",
      "----------\n",
      "Epoch Number: 17, Batch Number: 10, Training Loss: 26.5664\n",
      "Time so far is 0m 6s\n",
      "Epoch Number: 17, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 18/99\n",
      "----------\n",
      "Epoch 19/99\n",
      "----------\n",
      "Epoch Number: 19, Batch Number: 10, Training Loss: 26.5095\n",
      "Time so far is 0m 6s\n",
      "Epoch Number: 19, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 20/99\n",
      "----------\n",
      "Epoch 21/99\n",
      "----------\n",
      "Epoch Number: 21, Batch Number: 10, Training Loss: 26.5095\n",
      "Time so far is 0m 6s\n",
      "Epoch Number: 21, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 22/99\n",
      "----------\n",
      "Epoch 23/99\n",
      "----------\n",
      "Epoch Number: 23, Batch Number: 10, Training Loss: 26.5095\n",
      "Time so far is 0m 7s\n",
      "Epoch Number: 23, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 24/99\n",
      "----------\n",
      "Epoch 25/99\n",
      "----------\n",
      "Epoch Number: 25, Batch Number: 10, Training Loss: 26.5095\n",
      "Time so far is 0m 7s\n",
      "Epoch Number: 25, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 26/99\n",
      "----------\n",
      "Epoch 27/99\n",
      "----------\n",
      "Epoch Number: 27, Batch Number: 10, Training Loss: 26.4809\n",
      "Time so far is 0m 9s\n",
      "Epoch Number: 27, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 28/99\n",
      "----------\n",
      "Epoch 29/99\n",
      "----------\n",
      "Epoch Number: 29, Batch Number: 10, Training Loss: 26.4523\n",
      "Time so far is 0m 9s\n",
      "Epoch Number: 29, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 30/99\n",
      "----------\n",
      "Epoch 31/99\n",
      "----------\n",
      "Epoch Number: 31, Batch Number: 10, Training Loss: 26.4523\n",
      "Time so far is 0m 9s\n",
      "Epoch Number: 31, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 32/99\n",
      "----------\n",
      "Epoch 33/99\n",
      "----------\n",
      "Epoch Number: 33, Batch Number: 10, Training Loss: 26.4523\n",
      "Time so far is 0m 10s\n",
      "Epoch Number: 33, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 34/99\n",
      "----------\n",
      "Epoch 35/99\n",
      "----------\n",
      "Epoch Number: 35, Batch Number: 10, Training Loss: 26.4523\n",
      "Time so far is 0m 11s\n",
      "Epoch Number: 35, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 36/99\n",
      "----------\n",
      "Epoch 37/99\n",
      "----------\n",
      "Epoch Number: 37, Batch Number: 10, Training Loss: 26.3924\n",
      "Time so far is 0m 12s\n",
      "Epoch Number: 37, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 38/99\n",
      "----------\n",
      "Epoch 39/99\n",
      "----------\n",
      "Epoch Number: 39, Batch Number: 10, Training Loss: 26.3924\n",
      "Time so far is 0m 12s\n",
      "Epoch Number: 39, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 40/99\n",
      "----------\n",
      "Epoch 41/99\n",
      "----------\n",
      "Epoch Number: 41, Batch Number: 10, Training Loss: 26.3924\n",
      "Time so far is 0m 12s\n",
      "Epoch Number: 41, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 42/99\n",
      "----------\n",
      "Epoch 43/99\n",
      "----------\n",
      "Epoch Number: 43, Batch Number: 10, Training Loss: 26.3924\n",
      "Time so far is 0m 13s\n",
      "Epoch Number: 43, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 44/99\n",
      "----------\n",
      "Epoch 45/99\n",
      "----------\n",
      "Epoch Number: 45, Batch Number: 10, Training Loss: 26.3615\n",
      "Time so far is 0m 14s\n",
      "Epoch Number: 45, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 46/99\n",
      "----------\n",
      "Epoch 47/99\n",
      "----------\n",
      "Epoch Number: 47, Batch Number: 10, Training Loss: 26.3306\n",
      "Time so far is 0m 15s\n",
      "Epoch Number: 47, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 48/99\n",
      "----------\n",
      "Epoch 49/99\n",
      "----------\n",
      "Epoch Number: 49, Batch Number: 10, Training Loss: 26.3306\n",
      "Time so far is 0m 15s\n",
      "Epoch Number: 49, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 50/99\n",
      "----------\n",
      "Epoch 51/99\n",
      "----------\n",
      "Epoch Number: 51, Batch Number: 10, Training Loss: 26.3306\n",
      "Time so far is 0m 15s\n",
      "Epoch Number: 51, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 52/99\n",
      "----------\n",
      "Epoch 53/99\n",
      "----------\n",
      "Epoch Number: 53, Batch Number: 10, Training Loss: 26.3306\n",
      "Time so far is 0m 17s\n",
      "Epoch Number: 53, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 54/99\n",
      "----------\n",
      "Epoch 55/99\n",
      "----------\n",
      "Epoch Number: 55, Batch Number: 10, Training Loss: 26.2676\n",
      "Time so far is 0m 17s\n",
      "Epoch Number: 55, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 56/99\n",
      "----------\n",
      "Epoch 57/99\n",
      "----------\n",
      "Epoch Number: 57, Batch Number: 10, Training Loss: 26.2676\n",
      "Time so far is 0m 18s\n",
      "Epoch Number: 57, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 58/99\n",
      "----------\n",
      "Epoch 59/99\n",
      "----------\n",
      "Epoch Number: 59, Batch Number: 10, Training Loss: 26.2676\n",
      "Time so far is 0m 18s\n",
      "Epoch Number: 59, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 60/99\n",
      "----------\n",
      "Epoch 61/99\n",
      "----------\n",
      "Epoch Number: 61, Batch Number: 10, Training Loss: 26.2676\n",
      "Time so far is 0m 18s\n",
      "Epoch Number: 61, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 62/99\n",
      "----------\n",
      "Epoch 63/99\n",
      "----------\n",
      "Epoch Number: 63, Batch Number: 10, Training Loss: 26.2346\n",
      "Time so far is 0m 20s\n",
      "Epoch Number: 63, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 64/99\n",
      "----------\n",
      "Epoch 65/99\n",
      "----------\n",
      "Epoch Number: 65, Batch Number: 10, Training Loss: 26.2017\n",
      "Time so far is 0m 20s\n",
      "Epoch Number: 65, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 66/99\n",
      "----------\n",
      "Epoch 67/99\n",
      "----------\n",
      "Epoch Number: 67, Batch Number: 10, Training Loss: 26.2017\n",
      "Time so far is 0m 21s\n",
      "Epoch Number: 67, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 68/99\n",
      "----------\n",
      "Epoch 69/99\n",
      "----------\n",
      "Epoch Number: 69, Batch Number: 10, Training Loss: 26.2017\n",
      "Time so far is 0m 21s\n",
      "Epoch Number: 69, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 70/99\n",
      "----------\n",
      "Epoch 71/99\n",
      "----------\n",
      "Epoch Number: 71, Batch Number: 10, Training Loss: 26.2017\n",
      "Time so far is 0m 22s\n",
      "Epoch Number: 71, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 72/99\n",
      "----------\n",
      "Epoch 73/99\n",
      "----------\n",
      "Epoch Number: 73, Batch Number: 10, Training Loss: 26.1325\n",
      "Time so far is 0m 23s\n",
      "Epoch Number: 73, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 74/99\n",
      "----------\n",
      "Epoch 75/99\n",
      "----------\n",
      "Epoch Number: 75, Batch Number: 10, Training Loss: 26.1325\n",
      "Time so far is 0m 23s\n",
      "Epoch Number: 75, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 76/99\n",
      "----------\n",
      "Epoch 77/99\n",
      "----------\n",
      "Epoch Number: 77, Batch Number: 10, Training Loss: 26.1325\n",
      "Time so far is 0m 24s\n",
      "Epoch Number: 77, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 78/99\n",
      "----------\n",
      "Epoch 79/99\n",
      "----------\n",
      "Epoch Number: 79, Batch Number: 10, Training Loss: 26.1325\n",
      "Time so far is 0m 24s\n",
      "Epoch Number: 79, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 80/99\n",
      "----------\n",
      "Epoch 81/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 81, Batch Number: 10, Training Loss: 26.0962\n",
      "Time so far is 0m 25s\n",
      "Epoch Number: 81, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 82/99\n",
      "----------\n",
      "Epoch 83/99\n",
      "----------\n",
      "Epoch Number: 83, Batch Number: 10, Training Loss: 26.0600\n",
      "Time so far is 0m 26s\n",
      "Epoch Number: 83, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 84/99\n",
      "----------\n",
      "Epoch 85/99\n",
      "----------\n",
      "Epoch Number: 85, Batch Number: 10, Training Loss: 26.0600\n",
      "Time so far is 0m 26s\n",
      "Epoch Number: 85, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 86/99\n",
      "----------\n",
      "Epoch 87/99\n",
      "----------\n",
      "Epoch Number: 87, Batch Number: 10, Training Loss: 26.0600\n",
      "Time so far is 0m 27s\n",
      "Epoch Number: 87, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 88/99\n",
      "----------\n",
      "Epoch 89/99\n",
      "----------\n",
      "Epoch Number: 89, Batch Number: 10, Training Loss: 26.0600\n",
      "Time so far is 0m 28s\n",
      "Epoch Number: 89, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 90/99\n",
      "----------\n",
      "Epoch 91/99\n",
      "----------\n",
      "Epoch Number: 91, Batch Number: 10, Training Loss: 25.9845\n",
      "Time so far is 0m 28s\n",
      "Epoch Number: 91, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 92/99\n",
      "----------\n",
      "Epoch 93/99\n",
      "----------\n",
      "Epoch Number: 93, Batch Number: 10, Training Loss: 25.9845\n",
      "Time so far is 0m 29s\n",
      "Epoch Number: 93, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 94/99\n",
      "----------\n",
      "Epoch 95/99\n",
      "----------\n",
      "Epoch Number: 95, Batch Number: 10, Training Loss: 25.9845\n",
      "Time so far is 0m 29s\n",
      "Epoch Number: 95, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 96/99\n",
      "----------\n",
      "Epoch 97/99\n",
      "----------\n",
      "Epoch Number: 97, Batch Number: 10, Training Loss: 25.9845\n",
      "Time so far is 0m 30s\n",
      "Epoch Number: 97, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "Epoch 98/99\n",
      "----------\n",
      "Epoch 99/99\n",
      "----------\n",
      "Epoch Number: 99, Batch Number: 10, Training Loss: 25.9449\n",
      "Time so far is 0m 31s\n",
      "Epoch Number: 99, Batch Number: 10, Validation Metric: 0.0000\n",
      "Example output:\n",
      "\n",
      "Training complete in 0m 31s\n",
      "Epoch 0/4\n",
      "----------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Node' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4af33f174034>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                                  \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_criterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                  use_cuda=True)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TIME\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HarveyMuddWork/Neural_Nets_Research/neural_nets_research/neural_nets_library/training.py\u001b[0m in \u001b[0;36myet_another_train_func\u001b[0;34m(model, dset_loader, optimizer, lr_scheduler, num_epochs, print_every, plot_every, batch_size, validation_criterion, validation_dset, plateau_lr, use_cuda, save_file, save_folder, print_example, save_every)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdset_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mtree_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0minput_tree_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtree_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtree_tuple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m             \u001b[0mtarget_tree_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtree_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtree_tuple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HarveyMuddWork/Neural_Nets_Research/neural_nets_research/neural_nets_library/training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdset_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mtree_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0minput_tree_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtree_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtree_tuple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m             \u001b[0mtarget_tree_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtree_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtree_tuple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Node' object does not support indexing"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "best_model, train_plot_losses, validation_plot_losses, thing1, thing2 = training.train_model_tree_to_tree(program_model, for_lambda_dset, \n",
    "                                 optimizer, lr_scheduler=None, num_epochs=100, plot_every=200,\n",
    "#                                  batch_size=100, print_every=200,\n",
    "                                 batch_size=90, print_every=20, \n",
    "                                 validation_criterion=program_accuracy, use_cuda=True)\n",
    "\n",
    "best_model, train_plot_losses, validation_plot_losses = training.yet_another_train_func(program_model, for_lambda_dset, \n",
    "                                 optimizer, lr_scheduler=None, num_epochs=5, plot_every=1,\n",
    "                                 batch_size=90, print_every=20, validation_criterion=None,\n",
    "                                 use_cuda=True)\n",
    "end = datetime.datetime.now()\n",
    "print(\"TIME\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x * plot_every for x in range(len(train_plot_losses))], train_plot_losses)\n",
    "plt.show()\n",
    "\n",
    "plt.plot([x * plot_every for x in range(len(validation_plot_losses))], validation_plot_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plot_losses_old = train_plot_losses#_old + train_plot_losses_new\n",
    "validation_plot_losses_old = validation_plot_losses#_old + validation_plot_losses_new\n",
    "\n",
    "# import csv\n",
    "\n",
    "# torch.save(program_model, \"max-big-t2t-all-vars-model\")\n",
    "# with open(\"max-big-t2t-all-vars-train.txt\", \"w\") as output:\n",
    "#     writer = csv.writer(output, lineterminator='\\n')\n",
    "#     for val in train_plot_losses:\n",
    "#         writer.writerow([val]) \n",
    "# with open(\"max-big-t2t-all-vars-validation.txt\", \"w\") as output:\n",
    "#     writer = csv.writer(output, lineterminator='\\n')\n",
    "#     for val in validation_plot_losses:\n",
    "#         writer.writerow([val]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_ints + num_vars\n",
    "\n",
    "# Check whether a node is syntactically valid, given its parent and index\n",
    "# Then recursively do it for all the node's children\n",
    "def check_valid(node, parent, child_index):\n",
    "    category = parent_to_category_LAMBDA(parent, child_index, num_vars, num_ints)\n",
    "    possible_outputs = category_to_child_LAMBDA(category, num_vars, num_ints)\n",
    "    if not int(node.value) in possible_outputs:\n",
    "        print(\"parent\", parent, \"child_index\", child_index)\n",
    "        print(\"ERROR\", int(node.value), category)\n",
    "        return False\n",
    "    if (len(node.children) > 0):\n",
    "        child1 = check_valid(node.children[0], int(node.value), 0)\n",
    "        if not child1:\n",
    "            return False\n",
    "        child2 = check_valid(node.children[1], parent, child_index + 1)\n",
    "        if not child2:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Check all the programs in a dataset for syntactic accuracy\n",
    "# (this is a debugging function used to double check the accuracy of your grammar)\n",
    "def check_all():\n",
    "    i = 0\n",
    "    # Check grammar is right\n",
    "    for prog in for_lambda_dset:\n",
    "        correct = check_valid(prog[1], None, 0)\n",
    "        if correct is False:\n",
    "            print(i)\n",
    "            pretty_print_tree(prog[1])\n",
    "            return\n",
    "        i += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "check_all() #kangaroo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "notify_time": "10"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
