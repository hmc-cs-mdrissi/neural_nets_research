{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mehdi2277/Documents/HarveyMuddWork/Neural_Nets_Research/neural_nets_research\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from ANC.util import one_hotify\n",
    "from ANC.ANCDatasets import IncrementTaskDataset\n",
    "from neural_nets_library import training\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # Addition task\n",
    "# # Generate this by running the instructions here (but with the addition program file): https://github.com/aditya-khant/neural-assembly-compiler\n",
    "# # Then get rid of the .cuda in each of the tensors since we (or at least I) don't have cuda\n",
    "# init_registers = torch.IntTensor([6,2,0,1,0,0]) # Length R, should be RxM\n",
    "# first_arg = torch.IntTensor([4,3,3,3,4,2,2,5]) # Length M, should be RxM\n",
    "# second_arg = torch.IntTensor([5,5,0,5,5,1,4,5]) # Length M, should be RxM\n",
    "# target = torch.IntTensor([4,3,5,3,4,5,5,5]) # Length M, should be RxM\n",
    "# instruction = torch.IntTensor([8,8,10,5,2,10,9,0]) # Length M, should be NxM\n",
    "\n",
    "# Increment task\n",
    "init_registers = torch.IntTensor([6,0,0,0,0,0,0])\n",
    "first_arg = torch.IntTensor([5,1,1,5,5,4,6])\n",
    "second_arg = torch.IntTensor([6,0,6,3,6,2,6])\n",
    "target = torch.IntTensor([1,6,3,6,5,6,6])\n",
    "instruction = torch.IntTensor([8,10,2,9,2,10,0])\n",
    "\n",
    "# init_registers = torch.IntTensor([0,0,6,0,0,0]) ### Note that the paper has an Instruction Register on top\n",
    "# first_arg = torch.IntTensor([0,1,1,0,0,4,0]) ##\n",
    "# second_arg = torch.IntTensor([0,2,0,1,0,3,0]) ###\n",
    "# target = torch.IntTensor([1,5,1,5,0,5,5])\n",
    "# instruction = torch.IntTensor([8,10,2,9,2,10,0])\n",
    "\n",
    "# torch.Tensor{0, f, 6, 0, 0, f}\n",
    "# torch.Tensor{0, 1, 1, 0, 0, 4, f},  -- first arguments\n",
    "#    torch.Tensor{f, 2, f, 1, f, 3, f},  -- second arguments \n",
    "#    torch.Tensor{1, 5, 1, 5, 0, 5, 5},  -- target register !\n",
    "#    torch.Tensor{8,10, 2, 9, 2,10, 0}   -- instruction to operate OK\n",
    "\n",
    "# # Access task\n",
    "# init_registers = torch.IntTensor([0,0,0])\n",
    "# first_arg = torch.IntTensor([0,1,1,0,2])\n",
    "# second_arg = torch.IntTensor([2,2,2,1,2])\n",
    "# target = torch.IntTensor([1,1,1,2,2])\n",
    "# instruction = torch.IntTensor([8,2,8,9,0])\n",
    "\n",
    "\n",
    "# # Dummy Task\n",
    "# init_registers = torch.IntTensor([1])\n",
    "# first_arg = torch.IntTensor([0, 0])\n",
    "# second_arg = torch.IntTensor([0, 0])\n",
    "# target = torch.IntTensor([0, 0])\n",
    "# instruction = torch.IntTensor([1, 0])\n",
    "\n",
    "# # Dummy task - 0.5 stop prob\n",
    "# init_registers = torch.IntTensor([0, 0])\n",
    "# first_arg = torch.IntTensor([0, 1, 0])\n",
    "# second_arg = torch.IntTensor([0, 1, 0])\n",
    "# target = torch.IntTensor([0, 1, 0])\n",
    "# instruction = torch.IntTensor([1, 9, 0])\n",
    "\n",
    "\n",
    "\n",
    "# Get dimensions we'll need\n",
    "M = first_arg.size()[0]\n",
    "R = init_registers.size()[0]\n",
    "N = 11\n",
    "\n",
    "# Turn the given tensors into matrices of one-hot vectors.\n",
    "init_registers = one_hotify(init_registers, M, 0)\n",
    "first_arg = one_hotify(first_arg, R, 1)\n",
    "second_arg = one_hotify(second_arg, R, 1)\n",
    "target = one_hotify(target, R, 1)\n",
    "instruction = one_hotify(instruction, N, 1)\n",
    "\n",
    "# instruction[, ]\n",
    "\n",
    "\n",
    "# for i in range(R):\n",
    "#     first_arg[i, 6] = 1.0/6\n",
    "#     second_arg[i, 0] = 1.0/6\n",
    "#     second_arg[i, 2] = 1.0/6\n",
    "#     second_arg[i, 4] = 1.0/6\n",
    "#     second_arg[i, 6] = 1.0/6\n",
    "# for j in range(M):\n",
    "#     init_registers[1, j] = 1.0/7\n",
    "#     init_registers[5, j] = 1.0/7\n",
    "    \n",
    "# print(\"IR\", init_registers)\n",
    "# print(\"FA\", first_arg)\n",
    "# print(\"SA\", second_arg)\n",
    "# print(\"INST\", instruction)\n",
    "\n",
    "#octopus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def anc_validation_criterion(output, label):\n",
    "    output_mem = output[0].data\n",
    "    target_memory = label[0]\n",
    "    target_mask = label[1]\n",
    "    \n",
    "    output2 = output_mem * target_mask #\n",
    "    target_memory = target_memory * target_mask\n",
    "    _, target_indices = torch.max(target_memory, 2) #\n",
    "    _, output_indices = torch.max(output2, 2) #\n",
    "    return 1 - torch.equal(output_indices, target_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_examples = 7200\n",
    "\n",
    "plot_every = 10\n",
    "\n",
    "# M = 8 # Don't change this (as long as we're using the add-task)\n",
    "# dataset = AddTaskDataset(M, num_examples)\n",
    "# dataset = TrivialAddTaskDataset(M, num_examples)\n",
    "\n",
    "M = 7 # Don't change this (as long as we're using the inc-task)\n",
    "dataset = IncTaskDataset(M, 5, num_examples)\n",
    "\n",
    "# M = 5\n",
    "# dataset = AccessTaskDataset(M, num_examples)\n",
    "\n",
    "# M = 5\n",
    "# dataset = TrivialAccessTaskDataset(M, num_examples)\n",
    "\n",
    "# M = 2\n",
    "# dataset = DummyDataset(M, num_examples)\n",
    "\n",
    "\n",
    "data_loader = data.DataLoader(dataset, batch_size = 1) # Don't change this batch size.  You have been warned.\n",
    "\n",
    "# Initialize our controller\n",
    "controller = Controller(first_arg = first_arg, \n",
    "                        second_arg = second_arg, \n",
    "                        output = target, \n",
    "                        instruction = instruction, \n",
    "                        initial_registers = init_registers, \n",
    "                        stop_threshold = .9, \n",
    "                        multiplier = 1,\n",
    "                        correctness_weight = 1, \n",
    "                        halting_weight = 5, \n",
    "                        efficiency_weight = 0.01, \n",
    "                        confidence_weight = 0.1, \n",
    "                        t_max = 50) \n",
    "\n",
    "# Learning rate is a tunable hyperparameter. The paper used 1 or 0.1.\n",
    "optimizer = optim.Adam(controller.parameters(), lr = 0.1)\n",
    "\n",
    "best_model, train_plot_losses, validation_plot_losses = training.train_model_anc(\n",
    "    controller, \n",
    "    data_loader,  \n",
    "    optimizer, \n",
    "    num_epochs = 1, \n",
    "    print_every = 10, \n",
    "    plot_every = plot_every, \n",
    "    deep_copy_desired = False, \n",
    "    validation_criterion = anc_validation_criterion, \n",
    "    batch_size = 1) # In the paper, they used batch sizes of 1 or 5\n",
    "    \n",
    "    #kangaroo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot([x * plot_every for x in range(len(train_plot_losses))], train_plot_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(controller.times)), controller.times)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x * plot_every for x in range(len(validation_plot_losses))], validation_plot_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a bunch of times\n",
    "num_trials = 20\n",
    "\n",
    "num_original_convergences = 0\n",
    "num_0_losses = 0\n",
    "num_better_convergences = 0\n",
    "otherPrograms = []\n",
    "\n",
    "num_examples = 100\n",
    "\n",
    "for i in range(num_trials):\n",
    "    print(\"Trial \", i)\n",
    "    \n",
    "    M = 5\n",
    "    dataset = AccessTaskDataset(M, num_examples)\n",
    "    data_loader = data.DataLoader(dataset, batch_size = 1) # Don't change this batch size.  You have been warned.\n",
    "\n",
    "    controller = Controller(first_arg = first_arg, \n",
    "                        second_arg = second_arg, \n",
    "                        output = target, \n",
    "                        instruction = instruction, \n",
    "                        initial_registers = init_registers, \n",
    "                        stop_threshold = .9, \n",
    "                        multiplier = 1,\n",
    "                        correctness_weight = 1, \n",
    "                        halting_weight = 5, \n",
    "                        efficiency_weight = 0.5, \n",
    "                        confidence_weight = 0.1, \n",
    "                        t_max = 50) \n",
    "    \n",
    "    best_model, train_plot_losses, validation_plot_losses = training.train_model_anc(\n",
    "        controller, \n",
    "        data_loader,  \n",
    "        optimizer, \n",
    "        num_epochs = 15, \n",
    "        print_every = 5, \n",
    "        plot_every = plot_every, \n",
    "        deep_copy_desired = False, \n",
    "        validation_criterion = anc_validation_criterion, \n",
    "        batch_size = 1) # In the paper, they used batch sizes of 1 or 5\n",
    "    \n",
    "    percent_orig = compareOutput()\n",
    "    if percent_orig > .99:\n",
    "        num_original_convergences += 1\n",
    "    end_losses = validation_plot_losses[-2:]\n",
    "    if sum(end_losses) < .01:\n",
    "        num_0_losses += 1\n",
    "    if percent_orig < .99 and sum(end_losses) < .01:\n",
    "        num_better_convergences += 1\n",
    "        otherPrograms.append((controller.output, controller.instruction, controller.first_arg, controller.second_arg, controller.registers))\n",
    "print(\"LOSS CONVERGENCES\", num_0_losses * 1.0 / num_trials)\n",
    "print(\"ORIG CONVERGENCES\", num_original_convergences * 1.0 / num_trials)\n",
    "print(\"BETTER CONVERGENCES\", num_better_convergences * 1.0 / num_trials)\n",
    "\n",
    "# penguin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
